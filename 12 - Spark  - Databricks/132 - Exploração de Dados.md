<<<<<<< HEAD
<<<<<<< HEAD
132. Exploração de Dados
Bom, agora a gente vai começar a executar o Spark propriamente dito.

Então, o primeiro tem alguns exemplos de transformação de dados do Spark e o segundo é um exemplo de machine learning, especificamente com regressão, então a gente vai importar os notebooks. Mas, claro, você também pode criar o notebook se você quiser até digitar o código em vez de simplesmente executar.

A Databricks é aqui alguns parâmetros, todos eles são necessários. O primeiro aqui eu digo que o arquivo tem um cabeçalho, ou seja, a primeira linha do arquivo e o nome das colunas. O segundo eu digo que o separador é ponto e vírgula. Isso também é importante porque o padrão é vírgula.
Ok, então você tem que informar que o padrão aqui é ponto de vírgula e o terceiro aqui também importante necessário, que é o schema.

Para que o próprio Python ele faça uma inferência dos tipos das colunas.
Se você não definir o ínfero schema, ele vai definir todas as colunas como texto. O schema a gente pode dar os nomes das colunas e a gente pode dar os tipos dos dados. Isso, do ponto de vista de performance, é melhor, porque a gente vai ter uma solução mais performática, porque Spark não vai ter que fazer essa inferência. E lembrando que quando está fora Spark, a gente está falando de uma ferramenta criada para processar grandes volumes de dados.

Lembrando que o Data Frame do Spark ele é imutável. 
=======
132. Exploração de Dados
Bom, agora a gente vai começar a executar o Spark propriamente dito.

Então, o primeiro tem alguns exemplos de transformação de dados do Spark e o segundo é um exemplo de machine learning, especificamente com regressão, então a gente vai importar os notebooks. Mas, claro, você também pode criar o notebook se você quiser até digitar o código em vez de simplesmente executar.

A Databricks é aqui alguns parâmetros, todos eles são necessários. O primeiro aqui eu digo que o arquivo tem um cabeçalho, ou seja, a primeira linha do arquivo e o nome das colunas. O segundo eu digo que o separador é ponto e vírgula. Isso também é importante porque o padrão é vírgula.
Ok, então você tem que informar que o padrão aqui é ponto de vírgula e o terceiro aqui também importante necessário, que é o schema.

Para que o próprio Python ele faça uma inferência dos tipos das colunas.
Se você não definir o ínfero schema, ele vai definir todas as colunas como texto. O schema a gente pode dar os nomes das colunas e a gente pode dar os tipos dos dados. Isso, do ponto de vista de performance, é melhor, porque a gente vai ter uma solução mais performática, porque Spark não vai ter que fazer essa inferência. E lembrando que quando está fora Spark, a gente está falando de uma ferramenta criada para processar grandes volumes de dados.

Lembrando que o Data Frame do Spark ele é imutável. 
>>>>>>> d9f734c334f4b5e550e5383e11b878ea9d28d555
=======
132. Exploração de Dados
Bom, agora a gente vai começar a executar o Spark propriamente dito.

Então, o primeiro tem alguns exemplos de transformação de dados do Spark e o segundo é um exemplo de machine learning, especificamente com regressão, então a gente vai importar os notebooks. Mas, claro, você também pode criar o notebook se você quiser até digitar o código em vez de simplesmente executar.

A Databricks é aqui alguns parâmetros, todos eles são necessários. O primeiro aqui eu digo que o arquivo tem um cabeçalho, ou seja, a primeira linha do arquivo e o nome das colunas. O segundo eu digo que o separador é ponto e vírgula. Isso também é importante porque o padrão é vírgula.
Ok, então você tem que informar que o padrão aqui é ponto de vírgula e o terceiro aqui também importante necessário, que é o schema.

Para que o próprio Python ele faça uma inferência dos tipos das colunas.
Se você não definir o ínfero schema, ele vai definir todas as colunas como texto. O schema a gente pode dar os nomes das colunas e a gente pode dar os tipos dos dados. Isso, do ponto de vista de performance, é melhor, porque a gente vai ter uma solução mais performática, porque Spark não vai ter que fazer essa inferência. E lembrando que quando está fora Spark, a gente está falando de uma ferramenta criada para processar grandes volumes de dados.

Lembrando que o Data Frame do Spark ele é imutável. 
>>>>>>> d9f734c334f4b5e550e5383e11b878ea9d28d555

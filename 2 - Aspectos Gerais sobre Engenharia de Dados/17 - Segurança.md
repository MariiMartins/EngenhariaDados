17. Segurança

Nesta aula, a gente vai falar um pouco alguns aspectos sobre segurança. A segurança, obviamente, não é uma responsabilidade exclusiva do engenheiro de dados, mas ele tem, sim, a responsabilidade compartilhada com relação aos aspectos de segurança dos dados dentro de um  DataLake.

Então, a primeira questão é que autenticação e autorização.
•	Alguns conceitos de autenticação e garantir que o usuário é quem ele alega ser. Então, se você diz que você é um usuário x a autenticação, ela tem que garantir a que você de fato é o usuário x e autorização. Uma vez, o usuário autenticado é garantir que o usuário acesse dados aos quais ele tem permissão. Então, você é o usuário X ok, então você está autenticado. Agora eu tenho que lhe dar autorização sobre os dados. Que você tem acesso, que você tem permissão, aos quais lhe foi permitida a acesso.
•	O HDFS Ele pode operar sem nenhum tipo de autenticação e autorização. Ou pode usar frameworks bastante conhecidos comuns como LDAP e o Active Directory e o Hadoop? Da versão dois ele evoluiu bastante nesse aspecto de segurança. Nas primeiras versões, ele não tinha nenhuma, praticamente nenhuma funcionalidade nesse quesito. Hoje, ele está com uma infraestrutura com funcionalidades relacionadas a segurança bastante avançadas.
•	A gente vai falar aqui no próximo slide do Kerberos que ele também pode autenticar tanto usuários como nome
Então, o Kerberos como é em inglês, ele é amplamente utilizado inclusive na plataforma Windows. Então, o Hadoop pode usar ele para autenticar usuários e para também autenticar processos rodando entre os nós de um cluster em Hadoop em nome de um usuário. 
Então, o Kerberos é um serviço de autenticação em ambiente distribuído. Ele faz a mediação entre cliente e servidor usando a autenticação TPP, que é o trust third party. Ele tem alguns componentes que é o Kerberos Authentication Autenticador Server A S, que fornece credenciais e armazena as chaves, e ele tem o servidor de concessão de tickets que fornece credenciais para serviços específicos. O outro componente dele é o servidor de administração que administra as chaves privadas,  então o Kerberos pode ser utilizado nas questões de autenticação em um ambiente distribuído.

Uma outra questão relacionada à segurança é o Data at Rest, através de um conceito que a gente já falou, que se refere aos dados que estão armazenados, que estão parados, que estão registrados num sistema de arquivos diferente dos dados que estão em trânsito, por exemplo, entre os nós ou entre nós e o servidor, ou entre o servidor e o cliente, enfim, eles estão armazenados em disco.
O HDFS nas versões mais atuais, ele oferece criptografia transparente feita pela aplicação. Você pode ainda criar zonas criptografadas ou você pode criptografar diretórios, arquivos e diretórios, ou seja, você não precisa criptografar o sistema de arquivos todo você pode criptografar áreas específicas. Dados podem ser acessados diretamente no HDFS, sem passar pelas camadas de autenticação e autorização da aplicação.
Pode ser estendida essa criptografia para os dados em trânsito.
Existe ainda um outro, uma outra funcionalidade que é o Hadoop Key Management Server (KMS) , que pode fazer a gestão das chaves utilizadas na criptografia. Apache Ranger é o software da fundação a APE Apache, que na verdade é um framework para gerir a segurança de dados de todos. Praticamente todos os componentes dos os softwares da plataforma Hadoop. Então ele permite fazer a gestão da segurança por produto, então você pode fazer de forma  granular por um aplicativo pelo, por exemplo, pelo Hive, pelo Storm, pelo Hbase  ou por recurso (Você pode controlar a segurança por arquivo, por pasta, por grupos e por usuário.). E ele possui incluído funcionalidades de auditoria que a gente vai falar um pouquinho mais adiante.
Bom, a gente falou da criptografia dos dados at Rest que estão gravados, parados em disco, mas também é, outra questão importante é a comunicação entre os nós. Então você tem um cluster quando você tem um cluster.Você vai ter nós que podem estar dispersos, inclusive fisicamente.
Essa comunicação é que, por padrão, a comunicação entre os nós não é criptografada por padrão, se recomenda que se utilize criptografia com SSL ou TSL isso é suportado e pode ser implementado de forma relativamente fácil. 

# Auditoria e Logs.
Então, se você tem um DataLake, com dados sensíveis, com dados importantes, provavelmente você vai querer saber o que está acontecendo. O que vem acontecendo com esses dados. Você precisa ter uma ferramenta que faça auditoria desses dados e que registre logs de uso. (Então que arquivos foram copiados, acessados, apagados, enfim).
Esse é um requisito fundamental de segurança então, existe aqui algumas alternativas.
O Scribe:  um serviço de agregação de logos de logos do Facebook e uma ferramenta gratuita.
O Log Stach: serviço de ingestão e transformação de logs mantido pela Elastic, que é a famosa empresa que mantém o Elastic Search
E o Log4J: É um aplicativo de logs muito utilizado principalmente em ferramentas Java e é mantido pela também mantido pela Fundação Apache. Os logs deles são bastante populares a arquitetura, o tipo de log dele é bastante conhecido e pode ser utilizado também no ambiente Hadoop, principalmente porque o ambiente Hadoop tem suas aplicações desenvolvidas em Java.

Outros aspectos, inclusão de nós e novos softwares no servidor. Então, a gente ouve sempre falar que para escalar uma aplicação distribuída com o Hadoop e basta adicionar um nó. Então, se você precisa de mais capacidade de processamento, você adiciona um ou mais nós ao seu cluster Mas e todas as configurações funcionais e de segurança que o nó deve conter? Então isso é uma questão crítica e outra coisa. Se, por exemplo, você precisa adicionar um outro software ao seu ecossistema, seu cluster, por exemplo, se você adotou uma ferramenta de auditoria e você precisa incluir,  instalar essa ferramenta de forma distribuída em todo o seu cluster. Então, esses são aspectos de segurança importantíssimos.

Então, imagine se você inclui um novo nó e ele não leva as configurações de segurança que o cluster tem. Você coloca lá, você pode estar colocando para dentro do seu ambiente de processamento de dados uma porta de entrada, por exemplo, para uma invasão para um ataque malicioso. A mesma coisa se você coloca um software que pode que pode causar algum problema, não necessariamente que ele vá causar um problema de segurança no sentido de ser atacado ou de criar uma brecha, mas no sentido que ele pode estabilizar o serviço o cluster.

A outra questão são as APIs. Então, todos esses sistemas do ecossistema, o ecossistema Hadoop que a gente estudou, eles fornecem APIs para você utilizar e consultar serviços diretamente. Essas APIs normalmente precisam ser protegidas de injeção de comandos de injeção de ataques de buffer overflow, de injeção de SQL e outros tipos usuais de servir de ataques através de uma aplicação numa rede ou na internet.

E, por fim, talvez aí o elemento mais importante, mais relevante no quesito segurança do que o do ecossistema Hadoop seja o Apache Sentry, que é uma solução granular e modular de segurança criada para o ecossistema Hadoop. Então, ele tem plug ins para o Hive, Solr, plug in para o HDFS e para a maioria dos produtos.
Ele é composto por um servidor,  por um data engine (Hive)Por exemplo, para o SQL Server é um plug in, então esse tende a ser a tendência que seja um dos principais, senão o principal produto relacionado à segurança no ecossistema Hadoop. Então aqui a gente deu uma pincelada geral em alguns aspectos de segurança que devem ser considerados nos seus projetos de big data.
14. DataLakes

Durante esta sessão, nós vamos falar sobre aspectos diversos que estão relacionados à engenharia de dados e a projetos de big data. Então, ele vai falar sobre DataLake, sobre projetos, sobre gestão de riscos e outros temas relacionados. Nesta aula, nós vamos falar um pouco sobre DataLake.

# Então, o que é um DataLake? 
Então a gente pode dizer que o DataLake, que é a versão Big Data, é versão moderna do DataWarehouse,que surgiu nos anos 90. Ele não é uma substituição do DataWarehouse, que surgiu nos anos 90, do DataWarehouse, de dados estruturados, tratados que normalmente ingerem dados de bancos, de dados relacionais, de sistemas transacionais.
Ele é uma versão que suporta que se adaptou ao grande volume de dados que passou a ser gerado pelo fenômeno que nós estudamos, que é conhecido, que é chamado de Big Data. Então, DataLake são armazéns corporativos de dados. O objetivo dele é ser simples, disponível, acessível e seguro. Obviamente que quando se fala de simplicidade, a gente está falando de estrutura. Porém, um DataLake, por ser um armazém de dados distribuído, ele pode se tornar bastante complexo do ponto de vista de configuração de manutenção de segurança. Então, ele pode ser um ambiente bastante complexo e ele tem que estar em conformidade com os aspectos de governança de dados da organização. Ele tem que estar em conformidade com a legislação relacionada normalmente, mas não obrigatoriamente DataLake estão implementados utilizando a tecnologia Hadoop sobre o sistema de arquivos HDFS.Quando a gente fala Hadoop, não quer dizer que todos os dados que estão em um DataLake são processados pelo software Hadoop. Quando a gente fala de Hadoop, a gente está falando de um ecossistema que inclui o sistema de arquivos distribuído.Então, o principal elemento é que quando a gente fala de DataLake e o HDFS, então os dados estão no sistema de arquivos distribuídos HDFS.Como eu falei normalmente, mas não obrigatoriamente.

Não é obrigatório que um DataLake seja criado utilizando o HDFS um DataLake ele ingere dados estruturados, não estruturados e semi-estruturado diferente lá do DataWarehouse clássico que suportava apenas dados estruturados.

Um DataLake que ele tem três estruturas principais. Ele pode ser divididos em várias estruturas, em várias áreas, em várias regiões, mas eles têm três áreas principais.
# Landing Layer: 
onde são ingeridos os dados brutos. Então voces lembram que a gente falou que uma diferença do DataLake de um DataWarehouse clássico é a diferença entre as letras o ETL e o ELT. Então o DataLake ele ingere os dados brutos. Ele não faz o tratamento aqui antes de fazer a carga, como é o caso do ETL. O DataLake ele faz a extração, a ingestão e depois o tratamento dos dados.
Então, no Landing Layer chegam os dados brutos, claro que isso não é uma regra geral obrigatória. Eventualmente, existem alguns processos de ingestão que podem fazer algum tipo de tratamento e transformação de dados, mas ele é sempre mais superficial. Ele não é um tratamento de dados, é uma transformação de dados tão intensa como é feita no DataWarehouse clássico.
Landing Layer é um lugar que, entre aspas, não levando ao pé da letra que os dados são ingeridos da forma como eles são produzidos, da forma que eles são extraídos da sua fonte de origem. Daquela mesma forma, eles são colocados at rest eles são disponibilizados nesta camada.

# Analytc Layer:  
onde têm os dados tratados para consumo. Então aqui a gente tem o processo é a transformação. Nessa  Analytic Layer é onde estão os dados, por exemplo, que o cientista de dados vai utilizar para criar o modelo de machine learning. E onde estão os dados que vão ser carregados para o dashboard.Os dados para os relatórios, enfim, são os dados tratados prontos para consumo. As ferramentas de BI self-service e todo esse tipo de dado vai estar no analytic layer. E a outra estrutura principal que se encontra em um DataLake é:

# Área de Arquivamento:
que são os dados históricos, o que são dados históricos? Bom, você carrega dados, por exemplo, de um sistema transacional. Você tem uma visão, você tem uma fotografia, um snapshot da transação, da operação da empresa naquele instante, porém, a situação ela vai mudar os dados transacionais, elas vão mudar. O que você precisa fazer para manter esse histórico? Você precisa fazer o arquivamento desses dados mantendo o histórico.
Então você vai armazenando esses dados de forma histórica para caso você tenha alguma aplicação que utilize esses dados. É um formato histórico.Você tem esses dados arquivados, arquivados historicamente? A frequência com que ocorre esse arquivamento? Depende do tipo de aplicação. Depende da latência, do volume de dados, da necessidade da área de negócio.Então depende.

O mais comum é o chamado D+1 que você faz o arquivamento a cada dia. Mas, como eu falei, isso depende muito do tipo de negócio que está sendo arquivado.
Em termos de camadas um DataLake ele vai ter três principais camadas: 
•	a camada de aquisição que faz a extração dos dados;
•	a camada de processamento que faz a transformação, o processo de transformação dos dados;
•	e as camadas de consumo, que tem já os dados tratados prontos para serem  consumidos, por exemplo, pela Analytic Layer, e que são a camada onde os dados são consumidos, por exemplo, pelas ferramentas de BI e pelos analistas de negócio, pelo cientista de dados, enfim, por quem vai utilizar os dados.

Quanto a ingestão de dados, então existem ferramentas e processos para capturar dados de forma periódica para dentro do DataLake. Então, essa captura de dados ocorre de duas formas natch em, que são aqueles processos que são executados em lotes que não são em tempo real, então eles podem serem operacionalizados, por exemplo, uma vez por dia, uma vez por semana, uma vez por mês. Aí depende do tipo de negócio e os dados em real-time que são como a gente estudou lá, os sistemas de streaming, principalmente porque esses dados são carregados à medida que são produzidos.
Isso normalmente é feito por uma ferramenta que captura mudanças  nos sistemas de produção desses dados e faz a ingestão. Normalmente, esta captura é feita em sistemas transacionais e lá ele verifica dados que foram incluídos novos dados e dados que foram alterados.
Dados que foram incluídos normalmente ele identifica esses dados pelas chaves, pelas chaves primária dos dados e os dados que foram alterados por um campo que identifique alteração no registro, existem casos extremos, mas aí exige um processamento maior que ele faz a comparação (Existem scripts que fazem quando não existe a informação de data.) Ele faz a comparação dos registros, ver se existe alguma mudança no registro de forma inteira, célula por cela, coluna por coluna e se houver mudanças com relação a última captura, ele faz a nova captura dos dados. Então esta é a função camada. A função de injeção de dados DataLake, que é claro que é o DataLake que todo depende da ingestão de dados.
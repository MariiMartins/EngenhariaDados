<!--
CONTEÚDO ORIGINAL (EBOOKS)

## 1.2 - [Ebook] A IA não está eliminando Empregos. Está transformando Modelos de Negócios
A Inteligência Artificial (IA) tem o potencial de influenciar o mercado de trabalho, dependendo de sua utilização. Em certos casos, a IA pode automatizar tarefas previamente desempenhadas por seres humanos, o que pode resultar na substituição de trabalhadores por sistemas computacionais.

No entanto, a IA também gera oportunidades de emprego, como a necessidade de profissionais capazes de projetar, implementar e manter sistemas de IA. Além disso, a IA pode ser utilizada para aumentar a eficiência e produtividade em diversas atividades, permitindo que os trabalhadores se concentrem em tarefas mais complexas e criativas.

É importante ressaltar que a IA não é uma força autônoma que elimina empregos independentemente. Em vez disso, a IA consiste em um conjunto de ferramentas e tecnologias utilizadas por seres humanos para realizar tarefas de forma mais eficiente e eficaz.

O impacto da IA no emprego depende das decisões tomadas pelos seres humanos em relação à sua utilização, bem como da adaptação das políticas públicas e dos mercados de trabalho às mudanças causadas pela IA.

A IA pode ter um impacto significativo em vários modelos de negócios, mas isso não significa necessariamente a eliminação desses modelos. Ao contrário, a IA pode ser empregada para aprimorar e otimizar esses modelos de negócios, auxiliando as empresas a serem mais eficientes e a tomar melhores decisões.

Por exemplo, a IA pode automatizar tarefas repetitivas, liberando tempo para que os funcionários se dediquem a atividades de maior valor agregado. Além disso, ela pode analisar grandes volumes de dados e fornecer insights valiosos para a tomada de decisões empresariais.

No entanto, é importante lembrar que a IA também pode ter um impacto negativo em alguns modelos de negócios, como a substituição de trabalhadores humanos por robôs ou outras tecnologias de automação. Nesses casos, é fundamental que as empresas desenvolvam estratégias para garantir que os trabalhadores afetados sejam capacitados e realocados em novas posições, além de adotar medidas para mitigar quaisquer impactos negativos na economia e no mercado de trabalho.

## 2.5 - [Ebook] O futuro da Inteligência Artificial

Ao longo deste capítulo definimos Inteligência Artificial, conhecemos um pouco de sua história, áreas de pesquisa e campos relacionados, assim como a Sala Chinesa e o Teste de Turing. Estamos nitidamente num momento transitório, em que as tecnologias de IA estão evoluindo cada vez mais rápido e de forma irreversível. Mas e o futuro? O que mais avançado poderia ser feito com Inteligência Artificial? O objetivo deste texto é exatamente fazê-lo pensar no que está por vir. A verdade é que ainda estamos apenas no começo!

A ficção científica, por muitos anos, pregou um futuro em que os robôs seriam inteligentes e cyborgs seriam comuns. Filmes como "O Exterminador do Futuro", "Matrix", "Blade Runner", "Ex-Machina", "Her" e "Eu, Robô" são todos bons exemplos dessa visão.

Mas até a última década, a consideração do que isso poderia realmente significar no futuro era desnecessária porque era tudo ficção científica e não realidade científica. Agora, no entanto, a ciência não só conseguiu recuperar o atraso, como também introduziu aspectos práticos que permitem a aplicabilidade da IA em nossas vidas.

O que consideramos aqui são várias experiências diferentes ligando biologia e tecnologia em uma forma cibernética - essencialmente, em última análise, combinando seres humanos e máquinas em uma fusão relativamente permanente. Essa é uma linha de pesquisa que vem ganhando adeptos e evoluindo. O episódio 2 da terceira temporada da série "Black Mirror" na Netflix, retrata exatamente esse experimento. Assista se tiver oportunidade. O final é surpreendente. Outro exemplo é o excelente game de computador Titanfall 2, em que homem e máquina se ligam por um link neural, para formar um poderoso sistema de combate.

Quando normalmente pensamos em um robô, nós o consideramos simplesmente como uma máquina. Tendemos a pensar que ele pode ser operado remotamente por um ser humano, ou que pode ser controlado por um programa de computador simples.

Mas e se o robô tiver um cérebro biológico composto de células cerebrais, possivelmente até neurônios humanos? Os neurônios cultivados sob condições de laboratório em uma série de eletrodos não invasivos que fornecem uma alternativa atraente para realizar uma nova forma de controlador de robô. No futuro próximo, veremos robôs “pensantes” com cérebros não muito diferentes dos humanos. Você duvida?

Esse desenvolvimento levantará muitas questões sociais e éticas. Por exemplo, se o cérebro do robô tem aproximadamente o mesmo número de neurônios humanos que um cérebro humano típico, então poderia, ou deveria, ter direitos semelhantes aos de uma pessoa? Além disso, se esses robôs têm neurônios muito mais humanos do que em um cérebro humano típico - por exemplo, um milhão de vezes mais neurônios - eles, ao invés dos seres humanos, tomarão todas as decisões futuras?

Muitas interfaces computador / cérebro humano, são usadas para fins terapêuticos para superar problemas médicos ou neurológicos, com um exemplo, sendo os eletrodos de estimulação cerebral profunda, usados para aliviar os sintomas da doença de Parkinson. No entanto, mesmo aqui é possível considerar a utilização dessa tecnologia de forma que daria às pessoas habilidades que os seres humanos normalmente não possuem - em outras palavras, o aprimoramento humano. Em alguns casos, aqueles que sofreram amputações ou sofreram lesões na coluna devido a acidentes podem ser capazes de recuperar o controle através de seus sinais neurais ainda em funcionamento.

É claro que a conexão de um cérebro humano com uma rede de computadores através de um implante poderia, a longo prazo, abrir as vantagens distintas da inteligência da máquina, comunicação e habilidades de detecção para o indivíduo recebendo o implante. Atualmente, a obtenção de autorização para cada implantação requer aprovação ética da autoridade local que governa o hospital onde o procedimento é realizado. Mas, olhando para frente, é bem possível que as influências comerciais, juntamente com os desejos da sociedade para se comunicar de forma mais eficaz e perceber o mundo em uma forma mais rica, irá impulsionar o desejo do mercado.

Para alguns, as interfaces cérebro-computador são talvez um passo muito distante - particularmente se a abordagem significa interferência direta com o cérebro. Como resultado, a interface computador-cérebro mais estudada até o momento é a que envolve eletroencefalografia (EEG). Enquanto procedimentos de EEG possuem custo relativamente baixo, portátil e fácil de configurar, ainda é difícil ver a sua utilização futura de forma generalizada. Ele certamente tem um papel a desempenhar na avaliação externa de alguns aspectos do funcionamento do cérebro para fins médicos. No entanto, a ideia de pessoas dirigindo, enquanto usam um capacete de eletrodos, sem necessidade de um volante, não parece realista. Veículos completamente autônomos são muito mais prováveis.

Esses casos experimentais indicam como os seres humanos - e os animais, por assim dizer - podem se fundir com a tecnologia. Isso, por sua vez, gera um conjunto de considerações sociais e éticas, bem como questões técnicas. É por isso que é vital incluir um sentido de reflexão para que a experimentação adicional que agora testemunhamos seja guiada pelo feedback consciente

## 2.7 - [Ebook] Teste de Turing

As discussões contemporâneas sobre a natureza da mente são geralmente dominadas pelo que é conhecido como a concepção computacional, que identifica a mentalidade com a execução de programas: os seres humanos e as máquinas são supostos a operar de maneiras semelhantes. Talvez o mais importante representante desta posição seja Alan Turing, que introduziu o Turing Test (ou "TT") como um meio para determinar se as habilidades das máquinas eram comparáveis às dos seres humanos. A posição de Turing tem sido extremamente influente dentro da ciência cognitiva, que é dominada pelo modelo computacional da mente.

Embora o Teste de Turing tenha adquirido o status de conhecimento comum entre os estudantes de Inteligência Artificial e Ciência Cognitiva, seu caráter não é tão amplamente conhecido dentro da comunidade intelectual em geral. Turing adaptou um jogo, conhecido como o jogo de imitação (The Imitation Game), com a finalidade de provar a existência de inteligência ou mentalidade no caso de máquinas inanimadas. No jogo de imitação, um homem e uma mulher podem competir para induzir um competidor a adivinhar o que é feminino, baseado unicamente em respostas dadas a perguntas (permitindo que o macho, mas não a fêmea, minta).

O jogo precisava ser organizado de tal forma que as propriedades físicas dos participantes - suas formas, tamanhos e vozes, por exemplo - não tivessem influência. Então, se o competidor identificasse corretamente seu sexo, ele ou ela venceriam, mas de outra forma não. A concepção alternativa de Turing era adaptar o teste para colocar uma máquina inanimada contra um ser humano, onde a propriedade considerada não é mais a sexo dos participantes, mas sua inteligência ou mentalidade.

Para contornar o problema da falta de definição precisa para Inteligência Artificial, Alan Turing propôs em 1950, um teste capaz de determinar se uma máquina demonstra ou não inteligência (artificial), baseado no seguinte argumento:

“Não sabemos definir precisamente o que é inteligência e, consequentemente, não podemos de definir o que é inteligência artificial. Entretanto, embora não tenhamos uma definição de inteligência, podemos assumir que o ser humano é inteligente. Portanto, se uma máquina fosse capaz de se comportar de tal forma que não pudéssemos distingui-la de um ser humano, essa máquina estaria demonstrando algum tipo de inteligência que, nesse caso, só poderia ser inteligência artificial.”

Em 1950, Alan Turing publicou um artigo chamado “Computing Machine and Intelligence” (link do artigo na seção de link úteis). Neste artigo, Turing apresentou, pela primeira vez, o que hoje é conhecido por Teste de Turing, com o qual se pretendia descobrir se uma máquina podia ou não emular o pensamento humano. O Teste de Turing funciona da seguinte forma: um interrogador (humano) fará perguntas a duas entidades ocultas; uma delas é um humano e a outra é um computador. A comunicação entre o interrogador e as entidades é feita de modo indireto, pelo teclado, por exemplo. O interrogador tentará, através do “diálogo” realizado entre ele e as entidades, decidir qual dos dois é o humano. O computador será programado para se passar por humano e o humano responderá de forma a confirmar a sua condição. Se, no final do teste, o interrogador não conseguir distinguir quem é o humano, então conclui-se que o computador pode “pensar” segundo o Teste de Turing.

Em um de seus ensaios, Turing disse: “acredito que no fim do século o uso da palavra e a opinião, geralmente educada, terão se alterado tanto que alguém será capaz de falar de máquinas pensantes sem ser contraditado”. Porém já chegamos ao fim do século e entramos em outro e nenhuma máquina conseguiu passar, consistentemente, pelo Teste de Turing. Alguns computadores, devidamente programados, conseguiram passar por versões simplificadas do teste, contudo sempre esteve ausente o atributo mental do entendimento. Como Marvin Minsky, do MIT, disse: “o maior desafio é dar bom senso às máquinas, e bom senso é essencial para passar no Teste de Turing”. Russell e Norvig no clássico livro sobre Inteligência Artificial de 1995 (link na seção de bibliografia) observaram que programar um sistema de computador para passar no Teste de Turing é uma tarefa muito difícil. Tal sistema precisaria ter pelo menos as seguintes capacidades:

Processamento de linguagem natural para se comunicar com o usuário.
Representação de conhecimento para armazenar o que sabe ou aprende.
Raciocínio automatizado para usar o conhecimento armazenado com a finalidade de responder perguntas ou tirar novas conclusões.
Aprendizado de máquina para se adaptar a novas circunstâncias, detectar e extrapolar padrões, a fim de atualizar o seu conhecimento armazenado.
O Teste de Turing pode ser realizado de várias maneiras diferentes. Aqui o sucesso de uma máquina em induzir o concorrente a tratá-lo como humano seria tomado como prova de sua inteligência ou engenho.

A abordagem de Turing parece também enquadrar-se na tradição do “behaviorismo”, não apenas porque ele propôs um teste comportamental para a existência da mentalidade de máquina, mas também porque passar no Teste de Turing é suposto ser suficiente para justificar a existência da mentalidade. A máquina responder às perguntas de modo a induzir o competidor a adivinhar equivocadamente que a máquina é o humano, por exemplo, é visto como evidência suficientemente forte para justificar a conclusão de que a máquina possui mentalidade. Se ela passa pelo Teste de Turing, então ela possui uma mente, de acordo com esta concepção. Nesse sentido, a atribuição da mentalidade funciona como um modo abreviado de linguagem para a descrição do comportamento, onde atribuir essa propriedade não é cientificamente insignificante.

Mas o Teste de Turing sempre sofreu críticas, uma vez que existem muitas maneiras de responder e interpretar as perguntas.

O Teste de Turing foi adaptado para o cinema em 2015, no filme Ex Machina
<http://www.imdb.com/title/tt0470752>

## 2.8 - [Ebook] A sala chinesa

Pesquisadores da área de Inteligência Artificial dita “forte” acreditam que um dia haverá algum programa de computador tão complexo e sofisticado que será capaz de reproduzir qualquer tipo de ação desempenhada por um cérebro, inclusive raciocinar, tomar decisões inteligentes, ter sentimentos e tudo mais que hoje se pensa ser exclusivo de seres vivos, como os humanos. Note-se que eles falam de programas (“software”) que, em princípio, poderiam ser implantados em qualquer tipo de computador (“hardware”) suficientemente poderoso. Já o pessoal da IA dita “fraca” acha que esse computador apenas simularia o cérebro.

O filósofo norte-americano John Searle, da Universidade de Berkeley nos EUA, afirma que nenhum programa de computador, por mais complexo e avançado, será capaz de pensar. Para justificar sua objeção, Searle criou um experimento mental que ficou conhecido como a Sala Chinesa.

A Sala Chinesa (em inglês: Chinese Room) é um argumento hipotético criado por John Searle, em 1980, empregado por este em sua obra para refutar os teóricos da Inteligência Artificial forte. Baseia-se na presunção de que a sintaxe (gramática) não é garantia de existência da semântica (sentido).

Suponha que você, que não sabe absolutamente nada de chinês, está dentro de uma sala fechada onde existem caixas com coleções de símbolos chineses. Para você, que só entende português, esses símbolos não têm qualquer significado. Mas, existe na sala um enorme manual de instruções escrito em português que ensina como manipular esses símbolos. Outra pessoa, conhecedora da língua chinesa, está fora da sala e organiza perguntas ou frases coerentes em chinês, sobre qualquer assunto, juntando símbolos em pacotes que são passados para você através de uma abertura. Seu trabalho será usar o manual de instruções para formar uma nova coleção de símbolos que produza uma resposta coerente (em chinês) às questões que recebeu pela abertura. Considerando que os símbolos e o manual sejam bem completos, a pessoa do lado de fora vai achar que está se comunicando com alguém que sabe chinês. Sendo que você, na verdade, não sabe absolutamente nada dessa língua.

A Sala Chinesa pode ser considerada como um “computador” que aparenta saber chinês sem saber. Da mesma forma, diz Searle, um computador que passa pelo famoso Teste de Turing aparenta pensar, mas não pensa, pois não dispõe dos processos mentais inerentes ao pensamento. Abaixo, a descrição do experimento original criado por John Searle (traduzido para o português).

   Acesse também este vídeo com o experimento sendo realizado por um repórter da BBC, na série “The Hunt for AI”: <https://www.youtube.com/watch?v=D0MD4sRHj1M>

John Searle descreve uma sala hipotética com uma pessoa, o operador. Vários cestos com papéis onde estão desenhados ideogramas chineses estão na sala, assim como um livro de regras, escrito em inglês, de como combinar os ideogramas chineses. A pessoa recebe por um guichê de entrada uma sequência de ideogramas; utilizando o livro de regras, combina esses ideogramas de entrada e alguns que estão nos cestos, compondo uma nova sequência, que é então passada para fora da sala através de um guichê de saída. O operador não sabe o que está fazendo; na verdade, ele está respondendo perguntas em chinês. Searle argumenta que há uma distinção essencial entre esse operador e uma pessoa que leia e entenda chinês e responda perguntas sem utilizar um livro de regras. O primeiro está apenas seguindo regras sintáticas, mas o último está associando semântica ao que está fazendo. Searle afirma que a segunda pessoa está fazendo mais do que a primeira, porque entende o que cada pergunta e resposta significa. Ele diz, corretamente, que computadores são simplesmente máquinas sintáticas, combinando símbolos seguindo regras predeterminadas. Assim sendo, um computador pode substituir o operador daquela sala. Mas seres humanos fazem mais, eles associam significado, semântica, ao que eles observam e pensam. Como ele diz: "Há algo mais em ter uma mente do que executar processos formais ou sintáticos." Consequentemente, computadores nunca poderão pensar, porque pensar envolve semântica. Programas não são suficientes para atribuir mentes a computadores. Infelizmente, ele toma significado e semântica de uma forma ingênua, e não elabora sobre esses conceitos. Searle define a seguinte premissa: ele diz que "cérebros geram mentes", quer dizer, mentes são meros resultados, consequências de nossos cérebros físicos. Uma vez que abandonamos esse ponto de vista, é possível elaborar mais sobre o que pode ser entendimento, significado e semântica. O ponto importante agora é que essa premissa não invalida o argumento de sua sala chinesa. De acordo com esse argumento, computadores nunca poderão pensar.

Inteligência Artificial, portanto, é a capacidade do computador de reconhecer padrões e objetos, em velocidade cada vez maior e com isso “simular” o cérebro humano, usando diversas técnicas computacionais, matemática e estatística.

Alguns proponentes da IA “forte” argumentam que a pessoa dentro da sala pode até não saber chinês, mas, a sala toda (a pessoa, as caixas de símbolos e o manual de instruções) sabe. Isto é, o “sistema” completo sabe chinês, como o computador que passa pelo Teste de Turing pode também ser considerado como “pensante”.

Searle não concorda com essa conclusão e argumenta que a sala é até desnecessária. A pessoa que está nela poderia, em princípio, ser dotada de uma memória excepcional e decorar tudo, símbolos e manual. A troca de símbolos poderia acontecer fora da sala, os dois humanos frente a frente. O “sistema”, ou o homem que decorou tudo, se apresentaria como se fosse um surdo-mudo, comunicando-se em chinês por esses conjuntos de símbolos. Mesmo assim, segundo Searle, não seria correto dizer que a pessoa saberia chinês. “Conversar” em chinês não equivale a “entender” chinês. Essa pessoa, ou um programa de computador em seu lugar, estaria usando apenas a “sintaxe” da língua chinesa, mas não teria acesso à sua “semântica”.

Um computador usual funciona em série, manipulando bits de forma sequencial. Mas, já existem computadores com processamento paralelo e uma arquitetura que tenta reproduzir a estrutura de um cérebro (as GPUs – Unidades de Processamento Gráfico – que estudaremos no curso de número 3 da Formação IA), com neurônios e sinapses matemáticas a ponto de serem modificadas à medida que a máquina vai recebendo novas informações. Em outras palavras, esse tipo de computador que usa uma rede neural artificial é capaz de “aprender” enquanto manipula os dados que recebe. Se um computador desse tipo for usado em uma Sala Chinesa de Searle, sua troca de informação com o humano chinês seria cada vez mais elaborada e perfeita. Nesse caso, poderíamos dizer que essa máquina acabaria “entendendo” chinês?

Searle protesta dizendo que esse exemplo contradiz os defensores da IA “forte”, pois depende não apenas do programa (o “software”). Assim mesmo, segundo ele, essa máquina nunca entenderia o chinês, apenas simularia esse entendimento. Um computador e seu programa, por mais sofisticados, apenas processam informações – e processamento de informação não equivale a pensar.

Na verdade, diz Searle, o pensamento consciente deriva, necessariamente, de processos físico-químicos que não são reproduzidos em nenhum computador. A mente, segundo ele, não pode ser dissociada do cérebro. Aliás, não apenas do cérebro, mas de todo o organismo que interage com o cérebro trocando informações enquanto troca moléculas biológicas. A ideia, usada por autores de ficção científica, de que seria possível “exportar” uma mente de um cérebro para uma máquina externa, é inteiramente equivocada, segundo o filósofo americano.

O matemático inglês Roger Penrose concorda com Searle e também acha que uma mente não pode ser reproduzida por um algoritmo. No entanto, ele acha que os processos mentais dependem de processos quânticos, como a superposição de estados. Esses processos, ausentes de sistemas ou máquinas clássicas, seriam essenciais para o funcionamento de uma mente consciente. Essa é outra vertente muito interessante da controvérsia sobre máquinas “pensantes” que vale a pena acompanhar.

## 3.1 - [Ebook] Aprendizagem Humana x Aprendizagem de Máquina
Aprendizagem Humana x Aprendizagem de Máquina

O Processo de Aprendizagem Humana é o mecanismo pelo qual o conhecimento é adquirido, compreendido e incorporado. Ele pode ser descrito como um conjunto de transformações mentais que ocorrem em resposta a estímulos ou experiências. Em geral, o processo de aprendizagem envolve três fases:

    Aquisição: durante esta fase, o indivíduo é exposto a nova informação ou estímulo e essa informação é processada e armazenada na memória.

    Retenção: essa fase se concentra na manutenção da informação adquirida. Isso inclui a capacidade de acessar a informação quando necessário e a capacidade de evocá-la para uso futuro.

    Recuperação: essa fase se concentra na utilização da informação adquirida. Isso inclui a capacidade de aplicar o conhecimento adquirido em novas situações ou problemas, e a capacidade de adaptar o conhecimento adquirido para se adequar a novos contextos.

Existem vários modelos e teorias que tentam explicar como o processo de aprendizagem ocorre, incluindo teorias cognitivas, neurobiológicas e comportamentais. Além disso, existem diferentes tipos de aprendizagem, como aprendizagem declarativa, não-declarativa, por condicionamento, por observação, entre outros.

Já o Processo de Aprendizagem em Inteligência Artificial (IA) se refere ao processo pelo qual os algoritmos de IA são treinados para realizar tarefas específicas. Em geral, os algoritmos de IA são treinados com grandes conjuntos de dados, conhecidos como dados de treinamento, e os parâmetros do algoritmo são ajustados de maneira que o algoritmo possa realizar a tarefa desejada com maior precisão e eficiência.

Existem vários tipos de aprendizagem de IA, incluindo aprendizado supervisionado, não-supervisionado e por reforço. Cada tipo de aprendizado é usado para resolver problemas diferentes e tem suas próprias vantagens e desvantagens.

Além deles, existem outras formas de aprendizado, como aprendizado por transferência e aprendizado profundo, que são uma combinação de diferentes tipos de aprendizado e têm aplicações específicas.

Em geral, o processo de aprendizagem de IA inclui as seguintes etapas:

    - Coleta e preparação dos dados.
    - Escolha do algoritmo e configuração dos parâmetros.
    - Treinamento do modelo com os dados de treinamento.
    - Avaliação do modelo com dados de teste.
    - Ajuste do modelo e retreinamento, se necessário.
    - Uso do modelo para fazer previsões sobre novos dados.

## 3.4 - [Ebook] O que é um Modelo de Machine Learning?

**O Que é Um Modelo de Machine Learning/Aprendizado de Máquina?**
Um modelo de aprendizado de máquina é uma expressão de um algoritmo que vasculha montanhas de dados para encontrar padrões ou fazer previsões. Alimentados por dados, os modelos de machine learning / aprendizado de máquina são os motores matemáticos da inteligência artificial.
Por exemplo, um modelo de aprendizado de máquina para visão computacional pode identificar carros e pedestres em um vídeo em tempo real. Já um modelo de aprendizado de máquina para processamento de linguagem natural pode traduzir palavras e frases.
Um modelo de aprendizado de máquina é uma representação matemática de objetos e suas relações entre si. Os objetos podem ser qualquer coisa, desde "curtidas" em uma postagem de rede social até moléculas em um experimento de laboratório.
Portanto, não há limite para o uso da Inteligência, as combinações são infinitas.
Os Cientistas de Dados e Engenheiros de IA criam inúmeros modelos de aprendizado de máquina para diferentes usos.

    A tecnologia não para de evoluir.

## 3.5 - [Ebook] Processo de Aprendizagem em Machine Learning

P**rocesso de Aprendizagem em Machine Learning**

O processo de aprendizagem em machine learning ocorre quando um modelo é exposto a um conjunto de dados de treinamento e aprende a fazer previsões precisas sobre novos dados.

Isso é realizado por meio de ajuste dos parâmetros do modelo usando algoritmos de otimização, com base na avaliação da acurácia das previsões feitas sobre o conjunto de dados de treinamento.

O ajuste dos parâmetros do modelo usando algoritmos de otimização é realizado através de uma combinação de dois passos:

Definir uma função de perda (ou erro): essa função mede a diferença entre as previsões feitas pelo modelo e os valores reais para cada exemplo no conjunto de treinamento.

Otimizar a função de perda: isso envolve encontrar os valores dos parâmetros que minimizam a função de perda. Isso pode ser feito usando vários algoritmos de otimização, como gradiente descente, conjunto quasi-Newton, entre outros.

Durante a otimização, o algoritmo faz iterações sobre os parâmetros, atualizando-as com base na derivada da função de perda em relação a cada parâmetro. A otimização termina quando a função de perda atinge um mínimo ou quando o número máximo de iterações é atingido. Ao final do processo, os parâmetros do modelo são ajustados para minimizar a perda ou erro sobre o conjunto de treinamento.

O objetivo é encontrar a combinação de parâmetros que minimiza a perda ou erro entre as previsões e os valores reais. Depois de treinado, o modelo pode ser usado para fazer previsões sobre novos dados.

## 3.6 - [Ebook] O que é Treinamento em Inteligência Artificial?

Treinamento, validação e teste são etapas fundamentais no desenvolvimento de modelos de inteligência artificial.

Treinamento é o processo de aprender os parâmetros do modelo a partir de um conjunto de dados de treinamento. O modelo é "ajustado" para que possa fazer previsões precisas com base nos dados de treinamento.

Os dados de treinamento são um conjunto de exemplos que são usados para "ensinar" ao modelo como fazer previsões precisas. Esses dados são usados para ajustar os parâmetros do modelo, de modo que ele possa fazer previsões precisas com base nesses dados.

Os dados de treinamento geralmente incluem entradas (também chamadas de recursos ou atributos) e saídas (também chamadas de rótulos ou alvos). As entradas são as características dos exemplos que o modelo usa para fazer previsões, enquanto as saídas são os valores que o modelo deve prever.

Os dados de treinamento também são conhecidos como datasets. Eles são essenciais para todos os modelos de aprendizado de máquina e os ajudam a fazer previsões mais assertivas para executar tarefas desejadas.

Assim, os dados de treinamento criam o modelo de aprendizado de máquina. O modelo analisa o conjunto de dados repetidamente para entender profundamente suas características e vai se ajustando para um melhor desempenho.

Por exemplo, se você estiver treinando um modelo para prever se uma pessoa tem diabetes, os dados de treinamento poderiam incluir entradas como idade, peso, pressão arterial e níveis de açúcar no sangue, e a saída seria uma previsão de se a pessoa tem ou não diabetes.

Os dados de treinamento podem ser classificados em duas categorias: dados rotulados e dados não rotulados.

Dados de treinamento rotulados são usados no aprendizado supervisionado. Ele permite que os modelos de Machine Learning aprendam as características associadas aos rótulos específicos, que podem ser usados para classificar pontos de dados mais recentes. Por exemplo, as imagens de frutas podem ser identificadas como: bananas, maçãs ou laranjas, isso indica que um modelo pode usar dados de imagem rotulados para entender as características de cada frutas específica e usar essas informações para agrupar novas imagens.

A rotulagem é um processo demorado, pois é necessário marcar ou rotular os pontos de dados. A coleta de dados rotulados é desafiadora e cara.

Já os dados não rotulados, são aqueles dados brutos ou dados que não são marcados com nenhum rótulo para identificar suas classificações, características ou propriedades. Dados não rotulados são usados em aprendizado de máquina não supervisionado, e os modelos de Machine Learning precisam encontrar padrões ou semelhanças nos dados para chegar as conclusões.

Com base no exemplo anterior de bananas, maçãs e laranjas, em dados de treinamento não rotulados, as imagens dessas frutas não serão rotuladas. O modelo terá que avaliar cada imagem observando suas características, como a cor e a forma.

Depois de analisar um número considerável de imagens, o modelo será capaz de diferenciar novas imagens, baseado nos tipos de frutas como bananas, maçãs ou laranjas. Nesse caso, o modelo não saberá que a fruta em particular se chama maçã, banana ou laranja, em vez disso, ele conhecerá as características necessárias para identificar cada fruta.

## 3.7 - [Ebook] O que é Validação em Inteligência Artificial?

Validação é o processo de avaliar como o modelo se sai com novos dados, que não foram usados durante o treinamento. Esse processo ajuda a identificar overfitting, que é quando o modelo "memoriza" os dados de treinamento, mas não é capaz de generalizar para novos casos.

Os dados de validação geralmente incluem entradas e saídas semelhantes aos dados de treinamento. A diferença é que esses dados nunca foram usados durante o treinamento, então eles fornecem uma medida da capacidade do modelo de generalizar para novos casos.

Os dados de validação são usados para ajudar a escolher os melhores hiperparâmetros do modelo, e para identificar overfitting.

Se você tiver uma quantidade limitada de dados, uma técnica chamada validação cruzada pode ser usada para estimar o desempenho do modelo. Este método envolve particionar aleatoriamente os dados de treinamento em vários subconjuntos e reservar um para avaliação.

É muito comum as pessoas usarem os termos "dados de teste" e "dados de validação" de forma intercambiável. A principal diferença entre os dois é que os dados de validação são usados para validar o modelo durante o treinamento, enquanto o conjunto de dados de teste é usado para testar o modelo após a conclusão do treinamento.

O conjunto de dados de validação dá ao modelo a primeira amostra de dados não vistos. No entanto, nem todos os cientistas de dados realizam uma verificação inicial usando dados de validação. Alguns costumam pular esta etapa e ir diretamente para os dados de teste. Mas isso não é aconselhável.

Em geral, os dados de validação são usados para ajudar a escolher a melhor configuração para o modelo antes de usar os dados de teste para avaliar a performance final do modelo.

## 3.8 - [Ebook] O que é Teste em Inteligência Artificial?

Teste é o processo de avaliar como o modelo se sai com um conjunto de dados ainda não visto, com o objetivo de medir a performance final do modelo. Isso é importante para verificar se o modelo é capaz de generalizar para dados desconhecidos.

Os dados de teste geralmente incluem entradas e saídas semelhantes aos dados de treinamento e validação. A diferença é que esses dados nunca foram usados durante o treinamento ou validação, então eles fornecem uma medida precisa da performance final do modelo.

Os dados de teste são usados para avaliar a precisão do modelo, a capacidade de generalizar para novos casos, e também para comparar a performance do modelo com outros modelos.

Os dados de teste precisam representar o conjunto de dados real e ser grande o suficiente para gerar previsões significativas.

Em ciência de dados, é comum dividir os dados em 80% para treinamento e 20% para teste.

Essa é a última etapa antes de colocar o modelo em produção, e é importante ter um conjunto de dados de teste que seja representativo do conjunto de dados que será usado para tomar decisões.

## 3.11 - [Ebook] O que é Overfitting?

Overfitting é um conceito em ciência de dados, que ocorre quando um modelo estatístico se ajusta exatamente aos seus dados de treinamento. Quando isso acontece, o algoritmo infelizmente não pode funcionar com precisão em dados não vistos, anulando seu propósito.

A generalização de um modelo para novos dados é o que nos permite usar algoritmos de aprendizado de máquina todos os dias para fazer previsões e classificar dados.

Quando os algoritmos de aprendizado de máquina são construídos, eles aproveitam um conjunto de dados de amostra para treinar o modelo.

No entanto, quando o modelo treina por muito tempo em dados de amostra ou quando o modelo é muito complexo, ele pode começar a aprender o “ruído” ou informações irrelevantes dentro do conjunto de dados.

Quando o modelo memoriza o ruído e se ajusta muito próximo ao conjunto de treinamento, o modelo se torna “superajustado” e é incapaz de generalizar bem para novos dados.

Se um modelo não puder generalizar bem para novos dados, ele não será capaz de realizar as tarefas de classificação ou previsão para as quais foi planejado.

Baixas taxas de erro e alta variância são bons indicadores de overfitting. Para evitar esse tipo de comportamento, parte do conjunto de dados de treinamento é normalmente reservada como o “conjunto de teste” para verificar o overfitting.

Se os dados de treinamento tiverem uma taxa de erro baixa e os dados de teste tiverem uma taxa de erro alta, isso indica overfitting.

## 3.12 - [Ebook] Como detectar e evitar Overfitting?

Para entender a precisão dos modelos de aprendizado de máquina, é importante testar a adequação do modelo. A validação cruzada K-fold é uma das técnicas mais populares para avaliar a precisão do modelo.

Na validação cruzada de k-fold, (fold = dobras), os dados são divididos em k subconjuntos de tamanhos iguais, também chamados de "dobras". Uma das dobras k atuará como o conjunto de teste, também conhecido como conjunto de validação ou conjunto de validação, e as dobras restantes treinarão o modelo.

Esse processo se repete até que cada uma das dobras tenha agido como uma dobra de retenção. Após cada avaliação, uma pontuação é retida e, quando todas as iterações são concluídas, é calculada a média das pontuações para avaliar o desempenho do modelo geral.

Embora o uso de um modelo linear nos ajude a evitar Overfitting, muitos problemas do mundo real são não lineares. Além de entender como detectar o overfitting, é importante entender como evitar o overfitting completamente. Abaixo estão algumas técnicas que você pode usar para evitar o overfitting:

Fazer Parada Antecipada: como mencionamos anteriormente, esse método busca pausar o treinamento antes que o modelo comece a aprender o ruído dentro do modelo. Essa abordagem corre o risco de interromper o processo de treinamento muito cedo, levando ao problema oposto de subajuste. Encontrar o “ponto ideal” entre underfitting e overfitting é o objetivo final aqui.

Treinar Com Mais Dados: Expandir o conjunto de treinamento para incluir mais dados pode aumentar a precisão do modelo, fornecendo mais oportunidades para analisar a relação dominante entre as variáveis de entrada e saída. Dito isso, esse é um método mais eficaz quando dados limpos e relevantes são injetados no modelo. Caso contrário, você poderia simplesmente continuar a adicionar mais complexidade ao modelo, fazendo com que ele superajustasse.

Aumentar os Dados: embora seja melhor injetar dados limpos e relevantes em seus dados de treinamento, às vezes dados ruidosos são adicionados para tornar um modelo mais estável. No entanto, esse método deve ser feito com moderação.

Fazer a Seleção de Recursos: ao construir um modelo, você terá vários parâmetros ou recursos que são usados para prever um determinado resultado, mas muitas vezes esses recursos podem ser redundantes. A seleção de recursos é o processo de identificar os recursos mais importantes nos dados de treinamento e, em seguida, eliminar os irrelevantes ou redundantes. Isso é comumente confundido com redução de dimensionalidade, mas é diferente. No entanto, ambos os métodos ajudam a simplificar seu modelo para estabelecer a tendência dominante nos dados.

Realizar a Regularização: Se o overfitting ocorre quando um modelo é muito complexo, faz sentido reduzirmos o número de recursos. Mas e se não soubermos quais entradas eliminar durante o processo de seleção de recursos? Se não soubermos quais recursos remover de nosso modelo, os métodos de regularização podem ser particularmente úteis. A regularização aplica uma “penalidade” aos parâmetros de entrada com coeficientes maiores, o que subsequentemente limita a quantidade de variação no modelo. Embora existam vários métodos de regularização, como regularização L1, regularização Lasso e dropout, todos procuram identificar e reduzir o ruído nos dados.

Utilizar Métodos Ensemble: Os métodos de aprendizado ensemble são compostos por um conjunto de classificadores, por exemplo, árvores de decisão,  e suas previsões são agregadas para identificar o resultado mais popular. Os métodos de ensemble mais conhecidos são bagging e boosting. No bagging, uma amostra aleatória de dados em um conjunto de treinamento é selecionada com substituição, o que significa que os pontos de dados individuais podem ser escolhidos mais de uma vez. Depois que várias amostras de dados são geradas, esses modelos são treinados independentemente e dependendo do tipo de tarefa, ou seja, regressão ou classificação,  a média ou a maioria dessas previsões produz uma estimativa mais precisa. Isso é comumente usado para reduzir a variação dentro de um conjunto de dados ruidoso.

## 3.13 - [Ebook] O que é Underfitting?

Underfitting é um cenário em ciência de dados em que um modelo de dados é incapaz de capturar a relação entre as variáveis de entrada e saída com precisão, gerando uma alta taxa de erro tanto no conjunto de treinamento quanto nos dados não vistos.

Isso geralmente ocorre quando um modelo é muito simples, o que pode ser resultado de um modelo que precisa de mais tempo de treinamento, mais recursos de entrada ou menos regularização. Assim como o overfitting, quando um modelo é subajustado, ele não consegue estabelecer a tendência dominante nos dados, resultando em erros de treinamento e baixo desempenho do modelo.

Se um modelo não puder generalizar bem para novos dados, ele não poderá ser aproveitado para tarefas de classificação ou previsão. A generalização de um modelo para novos dados é o que nos permite usar algoritmos de aprendizado de máquina todos os dias para fazer previsões e classificar dados.

Viés alto e variância baixa são bons indicadores de underfitting. Como esse comportamento pode ser visto ao usar o conjunto de dados de treinamento, os modelos subajustados geralmente são mais fáceis de identificar do que os superajustados.

Underfitting ocorre quando um modelo de aprendizado de máquina não é complexo o suficiente para capturar a relação entre os dados de treinamento. Isso pode acontecer quando o modelo é muito simples ou se os recursos de entrada não são relevantes para a saída desejada. Como resultado, o modelo tem uma precisão baixa tanto nos dados de treinamento quanto nos dados de teste. 

Underfitting é um problema oposto ao overfitting, e pode ser resolvido tornando o modelo mais complexo ou adicionando mais recursos relevantes ao modelo.

## 3.14 - [Ebook] Como evitar Underfitting?

Como podemos detectar o underfitting com base no conjunto de treinamento, podemos ajudar a estabelecer a relação dominante entre as variáveis de entrada e saída no início. Ao manter a complexidade adequada do modelo, podemos evitar o subajuste e fazer previsões mais precisas.

Abaixo estão três técnicas que podem ser usadas para reduzir o Underfitting:

    1. Diminuir Regularização:
    A regularização é normalmente usada para reduzir a variância com um modelo aplicando uma penalidade aos parâmetros de entrada com coeficientes maiores. Existem vários métodos diferentes, como regularização L1, regularização Lasso, dropout, etc., que ajudam a reduzir o ruído e os outliers dentro de um modelo. No entanto, se as características dos dados se tornarem muito uniformes, o modelo será incapaz de identificar a tendência dominante, levando ao underfitting. Ao diminuir a quantidade de regularização, mais complexidade e variação são introduzidas no modelo, permitindo um treinamento bem-sucedido do modelo.

    2. Aumentar a Duração do Treinamento:
    Como mencionado anteriormente, interromper o treinamento muito cedo também pode resultar em um modelo de underfitting. 
    Portanto, ao estender a duração do treinamento, isso pode ser evitado. No entanto, é importante estar ciente do overtraining e, subsequentemente, do overfitting. Encontrar o equilíbrio entre os dois cenários será fundamental.

    3. Fazer Seleção de Recursos:
    Com qualquer modelo, recursos específicos são usados para estabelecer um determinado resultado.
    Se não houver recursos preditivos suficientes, devem ser introduzidos mais recursos ou recursos com maior importância.

Por exemplo, em uma rede neural, você pode adicionar mais neurônios ocultos ou em uma floresta aleatória, pode adicionar mais árvores.

Esse processo injetará mais complexidade no modelo, gerando melhores resultados de treinamento.

## 4.2 - [Ebook] Por que IA e Deep Learning estão mudando nossas vidas - P1

Você percebeu ao longo dos últimos anos o incrível salto de qualidade em diversas tecnologias que usamos em nosso dia a dia? Se você possui um smartphone com reconhecimento de voz, provavelmente percebeu. Essa funcionalidade em nossos smartphones está melhor do que nunca. Com um simples comando de voz, fazemos ligações, acessamos a internet, fazemos busca e abrimos aplicativos. Nunca foi tão fácil.

Estamos conversando com nosso computador mais do que nunca. Assistiu o filme Her (http://www.imdb.com/title/tt1798709/)? Não estamos muito longe disso! Softwares como o Cortana da Microsoft, Siri da Apple, Alexa da Amazon, ChatGPT e recursos de reconhecimento de voz do Google, nos permitem interagir com o computador de forma simples. Um simples comando de voz e o computador faz o que pedimos.

Mas não é apenas no reconhecimento de voz que podemos ver o avanço. Em um mundo onde tiramos fotos o tempo todo, o reconhecimento de imagens tem mostrado avanços consideráveis. Podemos agora buscar e organizar coleções de fotos, mesmo que elas não tenham uma tag de identificação. E isso é feito em apenas alguns segundos.

Vamos pensar sobre isso. Para ser capaz de identificar imagens de cães, uma app precisa ser capaz de identificar qualquer tipo de cão, desde um Chihuahua até um Pastor Alemão e não pode se equivocar com imagens invertidas ou parcialmente obscuras, sob neblina ou neve, no sol ou na sombra. A app deve ainda ser capaz de excluir lobos e gatos. A app precisa identificar um cachorro usando apenas os pixels de uma imagem. Como isso pode ser possível?

Mas o avanço do reconhecimento de imagens não é algo apenas para reconhecer imagens dos doces cachorrinhos. Startups ligadas a área médica já trabalham em projetos que usam computadores para a leitura de raios-X de forma mais rápida e precisa que um radiologista faria, para diagnosticar câncer o quanto antes e de forma menos invasiva e assim acelerar a busca por uma cura. O reconhecimento de imagens está levando ainda a melhorias significativas em robótica, drones autônomos capazes de fazer entregas e carros self-driving, aqueles que o Uber começou a testar recentemente e que podem andar por aí sem a necessidade de um motorista. Ford, Tesla, Uber, Baidu e Google estão trabalhando em protótipos de carros self-driving que já estão circulando em vias públicas.

Mas o que muitas pessoas não percebem é que toda essa evolução utiliza na sua essência a mesma tecnologia. Tudo isso tem sido possível graças a uma técnica de Inteligência Artificial chamada de Deep Learning (aprendizagem profunda), ou como alguns preferem chamar Deep Neural Networks.

O que mais chama atenção sobre as redes neurais é que não foi necessário que um ser humano programasse explicitamente o computador para realizar tudo que você leu nos parágrafos anteriores. De fato, nenhum ser humano poderia. Computadores foram alimentados com algoritmos (pequenas peças de código), que foram expostos a Terabytes de dados (milhares de imagens ou gravações de voz) e então treinados. Após esse treinamento o algoritmo aprendeu a reconhecer os padrões e quando expostos a novos dados, os algoritmos são capazes de identificar por si próprios imagens, objetos, frases ou palavras. Não é incrível? Como disse o CEO da Nvidia: “Os computadores já podem ensinar a si mesmos. Nós temos software escrevendo software”.

Mas redes neurais não são novas. O conceito é da década de 50 e muitos dos algoritmos de redes neurais foram escritos entre as décadas de 80 e 90. Mas 2 fatores fizeram as redes neurais voltarem ao centro das atenções: primeiro, o maior poder de processamento dos computadores e segundo, algo que você já deve ter ouvido falar: Big Data. Sim, ele de novo. Essa imensidão de dados não estruturados vindos de imagens, vídeos, áudio, arquivos de texto, mídias sociais ou mesmo da sua geladeira. Capacidade de processamento e Big Data são essenciais para que as redes neurais funcionem adequadamente. Agora pense comigo: ainda estamos na infância do Big Data e a capacidade de processamento tende a dobrar a cada 2 anos. O que ainda vem pela frente??

A prova de que isso tem chamado atenção foi a explosão de Startups de Inteligência Artificial e o total de dinheiro investido pelos fundos de investimentos nessas empresas: mais de 1 bilhão de dólares.

Recentemente as líderes do segmento de Inteligência Artificial: IBM, Microsoft, Amazon, Facebook e Google se uniram para formar uma organização sem fins lucrativos chamada Partnership on AI, voltada para o estudo e desenvolvimento da Inteligência Artificial, pesquisas e discussão sobre ética e boas práticas.

Se algumas das empresas mais valiosas e mais inovadoras do mundo se uniram em torno deste assunto é bem provável que este seja o caminho a ser seguido. Concorda?

## 4.3 - [Ebook] Por que IA e Deep Learning estão mudando nossas vidas - P2

Em 2012, o Google tinha 2 projetos de Deep Learning em progresso. Sabe quantos são atualmente? Mais de 1000. Vou repetir: mais de 1.000. Esses projetos envolvem Gmail, Youtube, Android, Google Maps, Google Translation e carros self-driving. A IBM também está ativa e o IBM Watson a plataforma de Inteligência Artificial da IBM, possui agora mais de 30 componentes baseados em Deep Learning.

Outro sinal de como Deep Learning está mudando nossas vidas. Investidores que alguns anos atrás não sabiam do que se tratava este assunto, agora só discutem investimento se a Startup tiver alguma solução envolvendo Inteligência Artificial. Processamento de Linguagem Natural e reconhecimento de voz estão se tornando padrão. Em um futuro próximo, as pessoas não estarão mais “teclando” em seus computadores ou smartphones, nem se perdendo no meio de menus infinitos. As pessoas vão querer apenas pedir e o computador vai executar a ação, seja ela qual for. Assistiu o filme Her? Pois bem, assista. Este é o mundo no qual estamos prestes a mergulhar.

Algumas empresas já integraram Deep Learning em suas operações. Na Microsoft, a equipe de vendas está usando redes neurais para prospectar clientes e gerar sistemas de recomendação.

E essa revolução passa pela evolução do hardware. Até a Lei de Moore foi superada. As Unidades de Processamento Gráfico (GPU – Graphical Processing Units) fabricadas pela Nvidia, que inicialmente foram desenvolvidas para tornar a experiência de jogar vídeo game cada vez melhor, agora são usadas para processamento de Deep Learning. As GPU’s são de 20 a 50 vezes mais velozes no processamento de Deep Learning que as CPU’s, por conta da capacidade de processamento paralelo das GPU’s. A Nvidia tem apostado alto em Deep Learning e segundo dados recentes, a empresa dobrou seu faturamento neste segmento e boa parte disso graças ao Deep Learning. Percebeu? Estamos no meio de uma nova revolução tecnológica.

O ChatGPT um robô de inteligência artificial que está revolucionando a forma de resposta à perguntas tecnológicas, utiliza o GPT (Transformador pré-treinado generativo) da OpenAI que é um modelo de aprendizado profundo baseado na arquitetura Transformer.

No caso do GPT, ele usa redes neurais profundas para gerar texto, prevendo a próxima palavra em uma sequência com base na entrada fornecida a ele. O modelo é treinado em um grande corpus de dados de texto, o que permite aprender padrões de linguagem e gerar um texto coerente semelhante aos dados de treinamento.

## 4.5 - [Ebook] O que é Aprendizagem profunda?

Hoje a teoria e a prática do aprendizado de máquina estão passando por uma "revolução profunda", causada pela implementação bem-sucedida dos métodos de aprendizagem profundos, representando as redes neurais de terceira geração.

Ao contrário das redes clássicas de segunda geração utilizadas nos anos 80-90 do século passado, os novos paradigmas de aprendizagem resolveram uma série de problemas que limitavam a expansão e a implementação bem-sucedida das redes neurais tradicionais.

Redes treinadas com algoritmos de aprendizagem profundos não se destacam apenas pelos melhores métodos alternativos em precisão, mas em alguns casos revelaram rudimentos sobre a compreensão dos sentidos da informação de entrada.

O reconhecimento de imagem e análise de informações de texto são os exemplos mais brilhantes.

Hoje, os métodos industriais mais avançados da visão computacional e de reconhecimento de voz são baseados em redes profundas.

Gigantes da indústria de TI como Apple, Google, OpenAI estão empregando pesquisadores em desenvolvimento de redes neurais profundas.

## 4.6 - [Ebook] Como surgiu a Aprendizagem profunda?

Uma equipe de estudantes de pós-graduação da Universidade de Toronto, no Canadá, liderada pelo professor Geoffrey E. Hinton ganhou o primeiro prêmio em um concurso patrocinado pela Merck. Usando um conjunto limitado de dados, que descreve a estrutura química de 15 moléculas, o grupo de G. Hinton conseguiu criar e aplicar um sistema especial que define quais dessas moléculas tinha mais chance de ser um medicamento eficaz.

A peculiaridade desse trabalho foi que os desenvolvedores usaram uma rede neural artificial baseada na aprendizagem profunda. Como resultado, esse sistema conseguiu realizar cálculos e pesquisas com base em um conjunto muito limitado de dados de origem, considerando que o treinamento de uma rede neural normalmente requer uma quantidade significativa de informações colocadas no sistema.

O resultado da equipe de Hinton foi particularmente impressionante porque a equipe decidiu entrar no concurso no último minuto. Acrescentando isto, o sistema de aprendizagem profunda foi desenvolvido sem o conhecimento específico sobre como as moléculas se ligam aos seus objetivos. A implementação bem-sucedida de uma aprendizagem mais profunda foi mais uma conquista no desenvolvimento da inteligência artificial no ano de 2012.

Então, no verão de 2012, Jeff Dean e Andrew Y. Ng da Google apresentaram um novo sistema de reconhecimento de imagem com taxa de precisão de 15,8%, onde para treinar um sistema de cluster de 16.000 nós eles usaram a rede IMAGEnet contendo uma biblioteca de 14 milhões de fotos de 20.000 objetos diferentes.

Recentemente, um programa criado por cientistas suíços superou um ser humano no reconhecimento de imagens de sinais de trânsito. O programa vencedor identificou com precisão 99.46% das imagens em um conjunto de 50.000; a pontuação máxima em um grupo de 32 participantes humanos foi de 99.22% e a média para os seres humanos era de 98.84%.

Em outubro de 2012, Richard F. Rashid, um coordenador de programas científicos da Microsoft apresentou em uma conferência em Tianjin, China uma tecnologia de tradução simultânea do Inglês para Mandarim acompanhado por uma simulação de sua própria voz.

Todas estas tecnologias que demonstram um avanço no domínio da inteligência artificial são baseadas no método de aprendizagem profunda. A principal contribuição para a teoria da aprendizagem profunda está sendo feita pelo professor Hinton, o tataraneto de George Boole, um cientista Inglês, fundador dos computadores contemporâneos subjacentes a álgebra de Boole.

Os métodos comuns da teoria de aprendizagem profunda da aprendizagem de máquina, são os algoritmos especiais para a análise de informações de entrada a vários níveis de apresentação. A peculiaridade da nova abordagem é que o aprendizado profundo estuda o assunto até que ele encontre suficientes níveis de cunho informativo para dar conta de todos os fatores que podem influenciar os parâmetros do objeto em questão.

Desta forma, uma rede neural com base em tal abordagem requer menos informações de entrada para a aprendizagem e uma rede treinada é capaz de analisar as informações com um maior nível de precisão do que as redes neurais habituais. O professor Hinton e seus colegas afirmam que sua tecnologia é especialmente boa para a busca de peculiaridades em matrizes de informações multidimensionais bem estruturadas.

As tecnologias de inteligência artificial (IA), em particular o aprendizado profundo, são amplamente utilizados em diferentes sistemas, incluindo o assistente pessoal inteligente da Apple, Siri, com base nas tecnologias da Nuance Communications e no reconhecimento de endereços no Google Street View. No entanto, os cientistas estão estimando o sucesso nesta esfera com muito cuidado já que a história da criação de uma inteligência artificial está cheia de promessas otimistas e decepções.

Na década de 1960, os cientistas acreditavam que seriam necessários apenas 10 anos para criar uma inteligência artificial inteiramente caracterizada. Então, na década de 1980, houve uma onda de jovens empresas oferecendo uma "inteligência artificial pronta", seguido pela "era do gelo" nesta esfera, que durou até recentemente. Hoje amplas capacidades computacionais disponíveis em serviços de nuvem fornecem um novo nível de implementação da poderosa rede neural, usando uma nova base teórica e algorítmica.

Deve-se notar que as redes neurais, mesmo as de terceira geração, como as redes neurais convolucionais, auto-associadores, máquinas de Boltzmann, nada têm em comum com os neurônios biológicos, exceto o nome.

Visite o site de Geoffrey Hinton aqui: <http://www.cs.toronto.edu/~hinton/>
-->
<!--
## 4.8 - [Ebook] Redes Neurais Origem e Evolução - P1

## 4.9 - [Ebook] Redes Neurais Origem e Evolução - P2

## 4.10 - [Ebook] Redes Neurais Origem e Evolução - P3

## 4.12 - [Ebook] O neurônio Biológico

## 4.13 - [Ebook] Comunicação entre Neurônios

## 4.## 4 - [Ebook] Principais tipos de redes neurais Profundas

## 4.15 - [Ebook] Autoencoder

## 4.16 - [Ebook] GAN - Redes Adversárias Generativas

## 4.17 - [Ebook] CNN - Redes Neurais Convolucionais

## 4.18 - [Ebook] RNN - Redes Neurais Recorrentes

## 4.19 - [Ebook] Arquitetura de Redes Neurais Profundas
